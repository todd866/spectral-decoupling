\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}

% Theorem environments
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}

% Macros
\newcommand{\Clam}{C(\lambda^*)}
\newcommand{\R}{\mathbb{R}}

\title{Spectral Decoupling of Capacity and Entropy\\in Network Dynamics}

\author{Ian Todd\\
\small Coherence Dynamics\\
\small \texttt{ian@coherencedynamics.com}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Network science commonly uses entropy-based measures to characterize dynamical complexity, often implicitly treating high entropy as synonymous with high-dimensional dynamics. We establish that \emph{spectral capacity} (the count of slow Laplacian modes) and \emph{state entropy} are independent axes of network dynamical complexity, controlled by distinct features: topology governs capacity via eigenvalue density, while noise governs entropy. We define spectral capacity $\Clam$ as the count of non-trivial Laplacian modes with eigenvalues below a threshold $\lambda^*$, and demonstrate that networks with matched state entropy can differ dramatically in capacity, and vice versa. Simulations across canonical architectures (ring, small-world, modular, random, scale-free) show that each topology occupies a distinct position on the capacity axis, while noise moves systems along the entropy axis. These results provide a principled two-axis framework for network complexity.
\end{abstract}

\textbf{Keywords:} network dynamics, spectral capacity, entropy, Laplacian, complexity

\section{Introduction}

Complex networks exhibit rich dynamical behavior arising from the interplay of topology and local dynamics. A central question in network science is how to characterize the ``complexity'' of such dynamics. Two natural candidates are \emph{dimensional capacity}---how many slow modes the topology supports---and \emph{entropy}---how unpredictable the dynamics are. These are often conflated, with entropy used as a proxy for complexity writ large \citep{gomez2008entropy, braunstein2006laplacian}.

However, classical results in dynamical systems establish that dimension and entropy are fundamentally distinct. The correlation dimension \citep{grassberger1983measuring}, embedding dimension \citep{takens1981detecting, sauer1991embedology}, and Lyapunov dimension \citep{kaplan1979chaotic} characterize the \emph{geometry of the attractor}. In contrast, entropy measures characterize the \emph{distribution over that support} \citep{eckmann1985ergodic}. As Eckmann and Ruelle noted, ``the dimension of the attractor and its entropy are independent characteristics.''

\begin{remark}[Capacity vs.\ Dimension]
The spectral capacity $\Clam$ we define is a \emph{mode-counting measure}---the number of slow Laplacian modes below a threshold. It is \textbf{not} a fractal dimension in the Grassberger--Procaccia sense. We use ``capacity'' to emphasize that this measures how many persistent degrees of freedom the topology \emph{supports}, not the fractal structure of an attractor.
\end{remark}

We address how network topology interacts with capacity-entropy decoupling. Our contributions are:

\begin{enumerate}
    \item \textbf{Spectral capacity}: We define $\Clam$ as the count of non-trivial Laplacian modes with $\lambda_k < \lambda^*$, a purely topological measure (Section~\ref{sec:theory}).

    \item \textbf{Decoupling proposition}: We show that networks can have matched state entropy with different capacity, and vice versa (Section~\ref{sec:theorem}).

    \item \textbf{Phase portrait diagnostic}: We introduce the $(C, H)$ phase portrait showing that different topologies occupy distinct capacity positions (Section~\ref{sec:results}).
\end{enumerate}

\section{Theoretical Framework}
\label{sec:theory}

\subsection{Network Diffusion Dynamics}

Consider a connected graph $G = (V, E)$ with $n = |V|$ nodes. Let $L$ denote the normalized Laplacian:
\begin{equation}
    L = I - D^{-1/2} A D^{-1/2}
\end{equation}
with eigenvalues $0 = \lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_n \leq 2$.

We study linear diffusive dynamics:
\begin{equation}
    \label{eq:diffusion}
    x(t+1) = (I - \alpha L) x(t) + \eta(t)
\end{equation}
where $\alpha \in (0, 1)$ is the diffusion rate and $\eta(t) \sim \mathcal{N}(0, \sigma^2 I)$.

In the Laplacian eigenbasis, mode $k$ evolves as:
\begin{equation}
    c_k(t+1) = (1 - \alpha \lambda_k) c_k(t) + \tilde{\eta}_k(t)
\end{equation}
with relaxation time $\tau_k = 1/(\alpha \lambda_k)$. Small eigenvalues yield slow modes; large eigenvalues yield fast-decaying modes.

\subsection{Spectral Capacity}

\begin{definition}[Spectral Capacity]
\label{def:capacity}
The spectral capacity at threshold $\lambda^*$ is:
\begin{equation}
    \Clam = \big|\{k : 0 < \lambda_k < \lambda^*\}\big|
\end{equation}
the count of \emph{non-trivial} Laplacian modes below threshold. We exclude the trivial $\lambda_1 = 0$ mode (constant eigenvector).
\end{definition}

This captures how many slow modes the topology supports. For $\lambda^* = 0.1$:
\begin{itemize}
    \item \textbf{Ring lattices} have dense small-$\lambda$ spectrum $\Rightarrow$ high capacity
    \item \textbf{Random graphs} have large spectral gap $\Rightarrow$ low capacity (often $C = 0$)
    \item \textbf{Modular networks} have near-degenerate community modes $\Rightarrow$ intermediate capacity
\end{itemize}

Crucially, $\Clam$ depends \emph{only on topology}---it is invariant to noise $\sigma$.

\subsection{State Entropy Proxy}

For the stationary distribution of~\eqref{eq:diffusion}, the differential entropy scales with:
\begin{equation}
    H \propto \frac{1}{n} \log \det(\mathrm{Cov}[x]) = \frac{1}{n} \sum_i \log \lambda_i^{\mathrm{cov}}
\end{equation}
where $\lambda_i^{\mathrm{cov}}$ are covariance eigenvalues. We use the mean log covariance eigenvalue as a \emph{state entropy proxy}. This is \textbf{not} the entropy rate $h_\mu$ of the process; it characterizes the spread of the stationary distribution.

The state entropy proxy depends on \emph{noise level} $\sigma$, not topology directly.

\section{Capacity-Entropy Decoupling}
\label{sec:theorem}

\begin{proposition}[Spectral Decoupling]
\label{prop:decoupling}
For linear diffusion dynamics on graphs:
\begin{enumerate}
    \item \textbf{Iso-entropy, different capacity}: There exist graphs $G_1, G_2$ and noise levels $\sigma_1, \sigma_2$ such that state entropy is matched, but $C(\lambda^*; G_1) \neq C(\lambda^*; G_2)$.

    \item \textbf{Fixed capacity, variable entropy}: For any fixed graph $G$, varying $\sigma$ changes state entropy while $\Clam$ remains constant.
\end{enumerate}
\end{proposition}

\begin{proof}[Proof sketch]
(1) Ring graphs have eigenvalues clustered near zero; random graphs have large spectral gap (first non-trivial eigenvalue $\lambda_2 \gg 0$). At threshold $\lambda^* = 0.1$, rings have $C \approx 6$ while random graphs have $C = 0$. Tuning $\sigma$ can match their state entropies, but capacity differs.

(2) $\Clam = |\{k : 0 < \lambda_k < \lambda^*\}|$ is determined by topology alone. Varying $\sigma$ scales covariance eigenvalues, changing state entropy, but does not change which Laplacian modes are below threshold.
\end{proof}

\begin{corollary}
The capacity difference between topologies is bounded by their integrated eigenvalue density difference:
\begin{equation}
    |C(\lambda^*; G_1) - C(\lambda^*; G_2)| = |N_1(\lambda^*) - N_2(\lambda^*)|
\end{equation}
where $N_i(\lambda^*) = |\{k : 0 < \lambda_k^{(i)} < \lambda^*\}|$.
\end{corollary}

\section{Results}
\label{sec:results}

We validate the theory on canonical architectures: ring lattices ($k=6$ neighbors), Watts-Strogatz small-worlds, stochastic block models (4 communities), Erd\H{o}s-R\'enyi random graphs, and Barab\'asi-Albert scale-free networks. All have $n = 100$ nodes; $\alpha = 0.1$; $\lambda^* = 0.1$.

\subsection{Capacity-Entropy Phase Portrait}

Figure~\ref{fig:phase_portrait} shows the $(C, H)$ phase portrait. Each topology occupies a distinct horizontal position (capacity), while noise moves systems vertically (entropy). Key observations:

\begin{itemize}
    \item \textbf{Ring}: $C = 6$ (many slow modes from dense small-$\lambda$ spectrum)
    \item \textbf{Modular}: $C = 3$ (community modes near zero)
    \item \textbf{Small-world}: $C = 2$ (rewiring increases spectral gap)
    \item \textbf{Random/Scale-free}: $C = 0$ (large spectral gap, no modes below $\lambda^* = 0.1$)
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\textwidth]{../figures/phase_portrait.png}
    \caption{Capacity-entropy phase portrait. Each topology has fixed capacity (horizontal position); noise moves systems vertically. Topology and noise control orthogonal axes.}
    \label{fig:phase_portrait}
\end{figure}

\subsection{Iso-Entropy Comparison}

Figure~\ref{fig:isoentropic} demonstrates Proposition~\ref{prop:decoupling}(1). We tune $\sigma$ for each topology to match state entropy at $H \approx 1.0$. Despite matched entropy: Ring has $C = 6$, Modular has $C = 3$, Random has $C = 0$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{../figures/isoentropic_comparison.png}
    \caption{Iso-entropy, different capacity. Left: capacity differs substantially. Right: state entropy matched by tuning $\sigma$.}
    \label{fig:isoentropic}
\end{figure}

\subsection{Fixed Capacity, Variable Entropy}

Figure~\ref{fig:fixed_capacity} demonstrates Proposition~\ref{prop:decoupling}(2). For a fixed modular network ($C = 3$), varying $\sigma$ changes state entropy continuously while capacity remains constant.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{../figures/isodimensional_experiment.png}
    \caption{Fixed capacity, variable entropy. Capacity is constant (determined by topology); state entropy varies with noise.}
    \label{fig:fixed_capacity}
\end{figure}

\subsection{Spectral Mechanism}

Figure~\ref{fig:spectral} shows Laplacian eigenvalue distributions. The threshold $\lambda^* = 0.1$ (dashed line) separates slow from fast modes. Ring lattices have eigenvalues clustered near zero; random graphs have a spectral gap with no eigenvalues below 0.1.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{../figures/spectral_theory.png}
    \caption{Laplacian spectra. Shaded region shows modes contributing to capacity. Ring has many slow modes; random has none below threshold.}
    \label{fig:spectral}
\end{figure}

\section{Discussion}

\subsection{Implications}

Network complexity is two-dimensional: capacity (topological) and entropy (dynamical). Entropy-only characterizations miss the capacity axis. A network can have:
\begin{itemize}
    \item High entropy, low capacity: unpredictable dynamics in few modes (e.g., noisy random graph)
    \item Low entropy, high capacity: structured dynamics across many modes (e.g., coherent ring)
\end{itemize}

For \textbf{neural networks}, this distinguishes high-dimensional activity \citep{stringer2019high} from high signal diversity \citep{schartner2017increased}. For \textbf{network design}, topology selection matters for capacity; noise tuning matters for entropy.

\subsection{Relation to Prior Work}

The dimension-entropy distinction is classical \citep{eckmann1985ergodic, grassberger1983measuring}. Our contribution is making it network-native via Laplacian spectral analysis. Related work includes entropy rate on networks \citep{gomez2008entropy}, correlation dimension \citep{lacasa2013correlation}, and entrograms \citep{faccin2018entrograms}.

\subsection{Limitations}

We analyzed linear diffusion; nonlinear dynamics may couple capacity and entropy in certain regimes. The threshold $\lambda^*$ must be chosen appropriately for the network scale. Empirical estimation from finite data requires care.

\section{Conclusion}

Spectral capacity and state entropy are orthogonal axes of network complexity. Topology sets capacity through eigenvalue density; noise sets entropy. The $(C, H)$ phase portrait provides a diagnostic for characterizing network dynamical regimes.

\section*{Data Availability}

Code available at \url{https://github.com/todd866/spectral-decoupling}.

\bibliographystyle{abbrvnat}
\bibliography{references}

\end{document}
