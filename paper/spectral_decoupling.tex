\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}

% Theorem environments
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}

% Macros
\newcommand{\Clam}{C(\lambda^*)}
\newcommand{\R}{\mathbb{R}}

\title{Spectral Decoupling of Capacity and Entropy\\in Network Dynamics}

\author{Ian Todd\\
\small Coherence Dynamics\\
\small \texttt{ian@coherencedynamics.com}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
The distinction between geometric dimension and entropy is classical in dynamical systems theory, yet network science often implicitly conflates these axes when characterizing dynamical complexity. This tutorial paper makes the dimension-entropy distinction explicit for network dynamics via Laplacian spectral analysis. We define \emph{spectral capacity} $\Clam$ as the count of non-trivial Laplacian modes with eigenvalues below a threshold $\lambda^*$---a purely topological measure---and show analytically that it decouples from state entropy, which depends on noise level. The closed-form decomposition $H = 2\log\sigma + B(G)$ separates the noise contribution from the topology-dependent baseline. Simulations across canonical architectures (ring, small-world, modular, random, scale-free) validate the theory and illustrate the $(C, H)$ phase portrait as a diagnostic tool. We provide code and figures for pedagogical use.
\end{abstract}

\textbf{Keywords:} network dynamics, spectral capacity, entropy, Laplacian, complexity, tutorial

\section{Introduction}

Complex networks exhibit rich dynamical behavior arising from the interplay of topology and local dynamics. A central question in network science is how to characterize the ``complexity'' of such dynamics. Two natural candidates are \emph{dimensional capacity}---how many slow modes the topology supports---and \emph{entropy}---how unpredictable the dynamics are. These are often conflated, with entropy used as a proxy for complexity writ large \citep{gomez2008entropy, braunstein2006laplacian}.

However, classical results in dynamical systems establish that dimension and entropy are fundamentally distinct. The correlation dimension \citep{grassberger1983measuring}, embedding dimension \citep{takens1981detecting, sauer1991embedology}, and Lyapunov dimension \citep{kaplan1979chaotic} characterize the \emph{geometry of the attractor}. In contrast, entropy measures characterize the \emph{distribution over that support} \citep{eckmann1985ergodic}. As Eckmann and Ruelle noted, ``the dimension of the attractor and its entropy are independent characteristics.''

\begin{remark}[Capacity vs.\ Dimension]
The spectral capacity $\Clam$ we define is a \emph{mode-counting measure}---the number of slow Laplacian modes below a threshold. It is \textbf{not} a fractal dimension in the Grassberger--Procaccia sense. We use ``capacity'' to emphasize that this measures how many persistent degrees of freedom the topology \emph{supports}, not the fractal structure of an attractor.
\end{remark}

\paragraph{Scope and contribution.} This paper is a \emph{tutorial synthesis} that makes the classical dimension-entropy distinction network-native. The underlying insight is not new; our contribution is:
\begin{enumerate}
    \item A clean \textbf{spectral capacity} definition via Laplacian eigenvalue counting (Section~\ref{sec:theory});
    \item A \textbf{closed-form decomposition} of state entropy into noise and topology components, yielding a rigorous decoupling proof (Section~\ref{sec:theorem});
    \item The $(C, H)$ \textbf{phase portrait} as a visual diagnostic, with validated simulations and reproducible code (Section~\ref{sec:results}).
\end{enumerate}

We aim for pedagogical clarity rather than claiming conceptual novelty. The value is in making an important distinction explicit and computable for network practitioners.

\section{Theoretical Framework}
\label{sec:theory}

\subsection{Network Diffusion Dynamics}

Consider a connected graph $G = (V, E)$ with $n = |V|$ nodes. Let $L$ denote the normalized Laplacian:
\begin{equation}
    L = I - D^{-1/2} A D^{-1/2}
\end{equation}
with eigenvalues $0 = \lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_n \leq 2$.

We study linear diffusive dynamics:
\begin{equation}
    \label{eq:diffusion}
    x(t+1) = (I - \alpha L) x(t) + \eta(t)
\end{equation}
where $\alpha \in (0, 1)$ is the diffusion rate and $\eta(t) \sim \mathcal{N}(0, \sigma^2 I)$.

In the Laplacian eigenbasis, mode $k$ evolves as:
\begin{equation}
    c_k(t+1) = (1 - \alpha \lambda_k) c_k(t) + \tilde{\eta}_k(t)
\end{equation}
with relaxation time $\tau_k = 1/(\alpha \lambda_k)$. Small eigenvalues yield slow modes; large eigenvalues yield fast-decaying modes.

\subsection{Spectral Capacity}

\begin{definition}[Spectral Capacity]
\label{def:capacity}
The spectral capacity at threshold $\lambda^*$ is:
\begin{equation}
    \Clam = \big|\{k : 0 < \lambda_k < \lambda^*\}\big|
\end{equation}
the count of \emph{non-trivial} Laplacian modes below threshold. We exclude the trivial $\lambda_1 = 0$ mode (constant eigenvector).
\end{definition}

This captures how many slow modes the topology supports. For $\lambda^* = 0.1$:
\begin{itemize}
    \item \textbf{Ring lattices} have dense small-$\lambda$ spectrum $\Rightarrow$ high capacity
    \item \textbf{Random graphs} have large spectral gap $\Rightarrow$ low capacity (often $C = 0$)
    \item \textbf{Modular networks} have near-degenerate community modes $\Rightarrow$ intermediate capacity
\end{itemize}

Crucially, $\Clam$ depends \emph{only on topology}---it is invariant to noise $\sigma$.

\subsection{State Entropy Proxy}
\label{sec:entropy}

\paragraph{Stationarity.} The trivial mode ($\lambda_1 = 0$) has eigenvalue 1 in the transition matrix $I - \alpha L$, so with additive noise it becomes a random walk rather than a stationary process. We project out this mode by centering: $\tilde{x}(t) = x(t) - \bar{x}(t)$, restricting dynamics to the $(n-1)$ nontrivial modes where stationarity holds.

\paragraph{Closed-form entropy.} In the Laplacian eigenbasis, mode $k \geq 2$ is an AR(1) process with coefficient $a_k = 1 - \alpha\lambda_k$. The stationary variance is:
\begin{equation}
    \mathrm{Var}(c_k) = \frac{\sigma^2}{1 - a_k^2} = \frac{\sigma^2}{\alpha\lambda_k(2 - \alpha\lambda_k)}
\end{equation}

The \emph{state entropy proxy} is the mean log covariance eigenvalue over nontrivial modes:
\begin{equation}
    \label{eq:entropy_decomp}
    H = \frac{1}{n-1}\sum_{k=2}^{n} \log \mathrm{Var}(c_k) = 2\log\sigma - \frac{1}{n-1}\sum_{k=2}^{n} \log\big(\alpha\lambda_k(2 - \alpha\lambda_k)\big)
\end{equation}

This decomposition is key: noise contributes an \emph{additive shift} ($2\log\sigma$), while topology contributes a \emph{spectrum-dependent offset}. For fixed topology, $\sigma$ moves the system along the entropy axis; across topologies, the Laplacian spectrum sets the baseline entropy at any given $\sigma$.

\begin{remark}[Entropy terminology]
This is \textbf{not} the entropy rate $h_\mu$, which measures unpredictability per time step. Our proxy characterizes the \emph{spread} of the stationary distribution---differential entropy of the state, not temporal uncertainty.
\end{remark}

\begin{remark}[Capacity terminology]
We use ``spectral capacity'' to mean slow-mode count, distinct from channel capacity (information theory) or flow capacity (network optimization). The term emphasizes that topology \emph{supports} a certain number of persistent degrees of freedom.
\end{remark}

\section{Capacity-Entropy Decoupling}
\label{sec:theorem}

\begin{proposition}[Spectral Decoupling]
\label{prop:decoupling}
For linear diffusion dynamics~\eqref{eq:diffusion} on connected graphs with dynamics restricted to nontrivial modes:
\begin{enumerate}
    \item \textbf{Iso-entropy, different capacity}: For any two graphs $G_1, G_2$ with different capacities $C(\lambda^*; G_1) \neq C(\lambda^*; G_2)$, there exist noise levels $\sigma_1, \sigma_2$ such that $H(G_1, \sigma_1) = H(G_2, \sigma_2)$.

    \item \textbf{Fixed capacity, variable entropy}: For any fixed graph $G$, varying $\sigma$ changes state entropy $H$ continuously while $\Clam$ remains constant.
\end{enumerate}
\end{proposition}

\begin{proof}
From~\eqref{eq:entropy_decomp}, the state entropy proxy decomposes as:
\[
H(G, \sigma) = 2\log\sigma + B(G)
\]
where the \emph{baseline offset} $B(G) = -\frac{1}{n-1}\sum_{k=2}^{n} \log(\alpha\lambda_k(2 - \alpha\lambda_k))$ depends only on the Laplacian spectrum.

\textbf{(1)} Given two graphs with different spectra, their baseline offsets satisfy $B(G_1) \neq B(G_2)$ generically. Setting $\sigma_2 = \sigma_1 \exp\big((B(G_1) - B(G_2))/2\big)$ yields $H(G_1, \sigma_1) = H(G_2, \sigma_2)$. Meanwhile, $\Clam$ depends only on the count of eigenvalues below $\lambda^*$, which is independent of $\sigma$.

\textbf{(2)} Spectral capacity $\Clam = |\{k : 0 < \lambda_k < \lambda^*\}|$ is determined by topology alone. From the decomposition, $\partial H / \partial \sigma = 2/\sigma > 0$, so $H$ increases monotonically with $\sigma$ while $\Clam$ remains constant.
\end{proof}

This makes explicit that capacity differences between topologies are \emph{entirely determined} by their integrated eigenvalue densities $N_i(\lambda^*) = |\{k : 0 < \lambda_k^{(i)} < \lambda^*\}|$. Graph families with predictable spectral densities (e.g., $d$-regular graphs, stochastic block models) thus have predictable capacity scaling.

\section{Results}
\label{sec:results}

We validate the theory on canonical architectures: ring lattices ($k=6$ neighbors), Watts-Strogatz small-worlds, stochastic block models (4 communities), Erd\H{o}s-R\'enyi random graphs, and Barab\'asi-Albert scale-free networks. All have $n = 100$ nodes; $\alpha = 0.1$; $\lambda^* = 0.1$. Error bars show variation over 30 random graph realizations per topology class.

\subsection{Capacity-Entropy Phase Portrait}

Figure~\ref{fig:phase_portrait} shows the $(C, H)$ phase portrait. Each topology occupies a distinct horizontal position (capacity), while noise $\sigma \in [0.5, 3.0]$ moves systems vertically (entropy). Horizontal error bars indicate capacity variability across graph realizations. Key observations:

\begin{itemize}
    \item \textbf{Ring}: $C = 6$ (many slow modes from dense small-$\lambda$ spectrum)
    \item \textbf{Modular}: $C = 3$ (community modes near zero)
    \item \textbf{Small-world}: $C = 2$ (rewiring increases spectral gap)
    \item \textbf{Random/Scale-free}: $C = 0$ (large spectral gap, no modes below $\lambda^* = 0.1$)
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\textwidth]{../figures/phase_portrait.png}
    \caption{Capacity-entropy phase portrait. Each topology has fixed capacity (horizontal position); noise moves systems vertically along the $\sigma$ range shown. Error bars indicate capacity variability across graph realizations. Topology and noise control orthogonal axes.}
    \label{fig:phase_portrait}
\end{figure}

\subsection{Iso-Entropy Comparison}

Figure~\ref{fig:isoentropic} demonstrates Proposition~\ref{prop:decoupling}(1). We tune $\sigma$ for each topology to match state entropy at $H \approx 1.0$. Despite matched entropy: Ring has $C = 6$, Modular has $C = 3$, Random has $C = 0$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{../figures/isoentropic_comparison.png}
    \caption{Iso-entropy, different capacity. Left: capacity differs substantially. Right: state entropy matched by tuning $\sigma$.}
    \label{fig:isoentropic}
\end{figure}

\subsection{Fixed Capacity, Variable Entropy}

Figure~\ref{fig:fixed_capacity} demonstrates Proposition~\ref{prop:decoupling}(2). For a fixed modular network ($C = 3$), varying $\sigma$ changes state entropy continuously while capacity remains constant.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{../figures/isodimensional_experiment.png}
    \caption{Fixed capacity, variable entropy. Capacity is constant (determined by topology); state entropy varies with noise.}
    \label{fig:fixed_capacity}
\end{figure}

\subsection{Spectral Mechanism}

Figure~\ref{fig:spectral} shows the integrated density of states $N(\lambda) = |\{k : \lambda_k < \lambda\}|$ for each topology. The spectral capacity is the CDF value at threshold $\lambda^* = 0.1$ (minus the trivial mode). Ring lattices have dense eigenvalues near zero; random graphs have a large spectral gap.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{../figures/spectral_theory.png}
    \caption{Integrated density of states (eigenvalue CDF). Capacity $\Clam$ equals the CDF at threshold minus 1 (for the trivial mode). Ring has many slow modes; random has none below threshold.}
    \label{fig:spectral}
\end{figure}

\subsection{Threshold Sensitivity}

Figure~\ref{fig:capacity_curve} shows capacity as a function of threshold $\lambda^*$. This ``capacity curve'' reveals how topologies separate: ring lattices accumulate capacity rapidly at small $\lambda^*$, while random graphs remain at $C = 0$ until $\lambda^*$ exceeds the spectral gap ($\lambda_2 \approx 0.15$ for $n=100$, $p=0.15$ ER graphs).

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\textwidth]{../figures/capacity_curve.png}
    \caption{Capacity curve: $C(\lambda^*)$ as a function of threshold. Different topologies separate in the small-$\lambda^*$ regime. Default threshold $\lambda^* = 0.1$ shown.}
    \label{fig:capacity_curve}
\end{figure}

\section{Discussion}

\subsection{Implications}

Network complexity is two-dimensional: capacity (topological) and entropy (dynamical). Entropy-only characterizations miss the capacity axis. A network can have:
\begin{itemize}
    \item High entropy, low capacity: unpredictable dynamics in few modes (e.g., noisy random graph)
    \item Low entropy, high capacity: structured dynamics across many modes (e.g., coherent ring)
\end{itemize}

For \textbf{neural networks}, this distinguishes high-dimensional activity \citep{stringer2019high} from high signal diversity \citep{schartner2017increased}. For \textbf{network design}, topology selection matters for capacity; noise tuning matters for entropy.

\subsection{Relation to Prior Work}

The dimension-entropy distinction is classical \citep{eckmann1985ergodic, grassberger1983measuring}. This tutorial makes it network-native via Laplacian spectral analysis, but does not claim the distinction itself as novel. Related work includes entropy rate on networks \citep{gomez2008entropy}, which measures temporal unpredictability rather than state spread; visibility graph correlation dimension \citep{lacasa2013correlation}; and entrograms for community detection \citep{faccin2018entrograms}. The spectral gap literature \citep{chung1997spectral} provides the mathematical foundation for our capacity definition.

\subsection{Limitations}

We analyzed linear diffusion; nonlinear dynamics may couple capacity and entropy in certain regimes. The threshold $\lambda^*$ must be chosen appropriately---we recommend either fixing a timescale $\tau$ and setting $\lambda^* = 1/(\alpha\tau)$, or choosing $\lambda^*$ as a fixed percentile of the spectrum to enable cross-network comparisons. Empirical estimation from finite data requires care.

\section{Conclusion}

Spectral capacity and state entropy are orthogonal axes of network complexity. Topology sets capacity through eigenvalue density; noise sets entropy. The $(C, H)$ phase portrait provides a diagnostic for characterizing network dynamical regimes.

\section*{Data Availability}

Code available at \url{https://github.com/todd866/spectral-decoupling}.

\bibliographystyle{abbrvnat}
\bibliography{references}

\end{document}
